[workflow]
# required: local work dir
local_work_root = /home/ocr/odem-wrk-dir
# required: local log dir
local_log_dir = /home/ocr/odem/odem-log
# if records used as data container, define field mapping
data_fields = IDENTIFIER, INFO, STATE, STATE_TIME
# if record data requested via OAI-PMH, set oai_base_url
;oai_base_url = https://opendata.uni-halle.de/oai/dd

[monitoring]
enable = True
# executes complete workflow in monitored thread
# use it at own risk, fails from time-to-time to 
# finish largerer work loads (500+ pages)
live = False
polling_interval = 1
path_disk_usage = /<path-where-to-monitor>
factor_free_disk_space_needed = 2.0
max_vmem_percentage = 75
;max_vmem_bytes = 9000000000

[ocr]
# use OCRD parallel workflow or ocr-pipeline workflow with tesseract
# possible values: 
# OCRD_PAGE_PARALLEL : OCR-D- Phase 3
# ODEM_TESSERACT     : ocr-pipeline newspapers
workflow_type = OCRD_PAGE_PARALLEL
# how many OCR-D containers to start in parallel mode
n_executors = 8
# limit for each single page-wise container
docker_container_memory_limit = 4GiB
docker_container_timeout = 600
# optional account mapping inside container
# defaults to current user/group
;docker_container_user = 1000
# if running OCR-D-based workflow, specify image
ocrd_baseimage = ocrd/all:2023-02-07
ocrd_logging = resources/ocrd_logging.conf
ocrd_resources_volumes = /data/ocr/tesseract4/tessdata:/usr/local/share/ocrd-resources/ocrd-tesserocr-recognize
# if running Tesseract in the backend
# mapping of ISO-3 language codecs to tesseract model configurations
#
# Please note:
# * tesseract models must be accessible at runtime
# * final mapping xxx => yyy is used as test value!
#
model_mapping = ara: ara.traineddata,
                cze: ces.traineddata,
                dan: dan.traineddata,
                dut: nld.traineddata,
                eng: eng.traineddata,
                fre: fra.traineddata,
                ger: gt4hist_5000k.traineddata,
                grc: grc.traineddata,
                gre: grc.traineddata,
                heb: heb.traineddata,
                hun: hun.traineddata,
                ine: ind.traineddata,
                ita: ita.traineddata,
                lat: lat_ocr.traineddata,
                lav: lav.traineddata,
                nds: gt4hist_5000k.traineddata,
                per: fas.traineddata,
                pol: pol.traineddata,
                rus: rus.traineddata,
                slo: slv.traineddata,
                spa: spa.traineddata,
                swe: swe.traineddata,
                tur: tur.traineddata,
                urd: urd.traineddata,
                yid: yid.traineddata,
                xxx: yyy.traineddata
# depends on ocr system. if false use first model only, default: True
# needs to be "False" if u use "ocrd-kraken-recognize"
model_combinable = True
tesseract_model_rtl = ara.traineddata, fas.traineddata, heb.traineddata, ulb-fas.traineddata
# elements to be removed from final OCR output
strip_tags = alto:Shape,alto:Processing,alto:Illustration,alto:GraphicalElement
# defines the OCR-D Processing steps. Mandatory. https://ocr-d.de/en/workflows
ocrd_process_list = olena-binarize -I MAX -O OCR-D-BINPAGE -P impl sauvola-ms-split -P dpi 300,
                    anybaseocr-crop -I OCR-D-BINPAGE -O OCR-D-SEG-PAGE-ANYOCR -P dpi 300,
                    cis-ocropy-denoise -I OCR-D-SEG-PAGE-ANYOCR -O OCR-D-DENOISE-OCROPY -P level-of-operation page -P noise_maxsize 3.0 -P dpi 300,
                    tesserocr-segment-region -I OCR-D-DENOISE-OCROPY -O OCR-D-SEG-BLOCK-TESSERACT -P padding 5 -P find_tables false -P dpi 300,
                    segment-repair -I OCR-D-SEG-BLOCK-TESSERACT -O OCR-D-SEGMENT-REPAIR -P plausibilize true -P plausibilize_merge_min_overlap 0.7,
                    cis-ocropy-clip -I OCR-D-SEGMENT-REPAIR -O OCR-D-CLIP,
                    cis-ocropy-segment -I OCR-D-CLIP -O OCR-D-SEGMENT-OCROPY -P spread 2.4 -P dpi 300,
                    cis-ocropy-dewarp -I OCR-D-SEGMENT-OCROPY -O OCR-D-DEWARP,
                    tesserocr-recognize -I OCR-D-DEWARP -O PAGE -P textequiv_level {tesseract_level} -P model {model_config}

[mets]
# enable/disable METS validations
# be sure even problematic data
# can be used, but enforce only 
# to return hopefully fixed data
# ULB Sachsen-Anhalt defaults
prevalidate = True
postvalidate = True
validate = True
# structMapLogical_22 => DEFAULT fileGroup is missing => DSpace will re-generate
# fileSec_02          => DEFAULT fileGroup is missing => DSpace will re-generate
# originInfo_06       => mods:placeTerm@type="code" considered invalid (DDB drops this at import anyway)
ddb_validation_ignore = structMapLogical_22, fileSec_02, originInfo_06
# METS filegroups to be removed during processing
blacklist_file_groups = DEFAULT, THUMB, THUMBS, MIN, FULLTEXT, DOWNLOAD
# METS containers (both pages and logical structures) 
# to be ignored from ocr-ing
blacklist_logical_containers = cover_front,cover_back
blacklist_physical_container_labels = Auftragszettel,Colorchecker,Leerseite,RÃ¼ckdeckel,Deckblatt,Vorderdeckel,Illustration,Karte
agents = ODEM

[derivans]
# configure optional OCR-assets
derivans_enabled = False
derivans_image = ghcr.io/ulb-sachsen-anhalt/digital-derivans:latest
derivans_config = bin/config/derivans.ini
derivans_logdir = /home/ocr/odem/odem-log/

# configure optional server/client
# mechanics if run in client/server-mode
[record-server]
record_server_url = <SERVICE-IP>
record_server_port = <SERVICE-PORT>
record_server_resource_dir = <RECORD-LIST-DIR>

[export]
# configure optional export actions and formats
export_enabled = False
local_export_tmp = /home/ocr/odem-tmp-export
local_export_dir = /data/ktopro-opendata/odem
delete_subdirs_before_export = MAX, IMAGE_80, PAGE
export_collection = 123456/789
export_mappings = .xml: mets.xml, .pdf: , FULLTEXT:
# possible formats SAF, FLAT_ZIP (default: SAF)
export_format = SAF
# (default: True)
# include (probably modified) METS into SAF
export_mets = True
# (default: True)
enrich_mets_fulltext = True
# optional: OCR txt-file required
create_textline_asset = True
